gpu_info:
[32mcuda.is_available:True
[32mcuda.device_count:1
[32mcuda.device_name:NVIDIA GeForce RTX 3050 Laptop GPU
[32mcuda.current_device:0
datasets_info:
[32mtrain_inputs:50000	train_labels:50000
[32mtrain_inputs:10000	train_labels:10000
åŠ è½½ç½‘ç»œç»“æ„
net_structure:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 16, 16]             432
       BatchNorm2d-2           [-1, 16, 16, 16]              32
         Hardswish-3           [-1, 16, 16, 16]               0
            Conv2d-4             [-1, 16, 8, 8]             144
       BatchNorm2d-5             [-1, 16, 8, 8]              32
              ReLU-6             [-1, 16, 8, 8]               0
 AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0
            Conv2d-8              [-1, 8, 1, 1]             136
              ReLU-9              [-1, 8, 1, 1]               0
           Conv2d-10             [-1, 16, 1, 1]             144
      Hardsigmoid-11             [-1, 16, 1, 1]               0
SqueezeExcitation-12             [-1, 16, 8, 8]               0
           Conv2d-13             [-1, 16, 8, 8]             256
      BatchNorm2d-14             [-1, 16, 8, 8]              32
 InvertedResidual-15             [-1, 16, 8, 8]               0
           Conv2d-16             [-1, 72, 8, 8]           1,152
      BatchNorm2d-17             [-1, 72, 8, 8]             144
             ReLU-18             [-1, 72, 8, 8]               0
           Conv2d-19             [-1, 72, 4, 4]             648
      BatchNorm2d-20             [-1, 72, 4, 4]             144
             ReLU-21             [-1, 72, 4, 4]               0
           Conv2d-22             [-1, 24, 4, 4]           1,728
      BatchNorm2d-23             [-1, 24, 4, 4]              48
 InvertedResidual-24             [-1, 24, 4, 4]               0
           Conv2d-25             [-1, 88, 4, 4]           2,112
      BatchNorm2d-26             [-1, 88, 4, 4]             176
             ReLU-27             [-1, 88, 4, 4]               0
           Conv2d-28             [-1, 88, 4, 4]             792
      BatchNorm2d-29             [-1, 88, 4, 4]             176
             ReLU-30             [-1, 88, 4, 4]               0
           Conv2d-31             [-1, 24, 4, 4]           2,112
      BatchNorm2d-32             [-1, 24, 4, 4]              48
 InvertedResidual-33             [-1, 24, 4, 4]               0
           Conv2d-34             [-1, 96, 4, 4]           2,304
      BatchNorm2d-35             [-1, 96, 4, 4]             192
        Hardswish-36             [-1, 96, 4, 4]               0
           Conv2d-37             [-1, 96, 2, 2]           2,400
      BatchNorm2d-38             [-1, 96, 2, 2]             192
        Hardswish-39             [-1, 96, 2, 2]               0
AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0
           Conv2d-41             [-1, 24, 1, 1]           2,328
             ReLU-42             [-1, 24, 1, 1]               0
           Conv2d-43             [-1, 96, 1, 1]           2,400
      Hardsigmoid-44             [-1, 96, 1, 1]               0
SqueezeExcitation-45             [-1, 96, 2, 2]               0
           Conv2d-46             [-1, 40, 2, 2]           3,840
      BatchNorm2d-47             [-1, 40, 2, 2]              80
 InvertedResidual-48             [-1, 40, 2, 2]               0
           Conv2d-49            [-1, 240, 2, 2]           9,600
      BatchNorm2d-50            [-1, 240, 2, 2]             480
        Hardswish-51            [-1, 240, 2, 2]               0
           Conv2d-52            [-1, 240, 2, 2]           6,000
      BatchNorm2d-53            [-1, 240, 2, 2]             480
        Hardswish-54            [-1, 240, 2, 2]               0
AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0
           Conv2d-56             [-1, 64, 1, 1]          15,424
             ReLU-57             [-1, 64, 1, 1]               0
           Conv2d-58            [-1, 240, 1, 1]          15,600
      Hardsigmoid-59            [-1, 240, 1, 1]               0
SqueezeExcitation-60            [-1, 240, 2, 2]               0
           Conv2d-61             [-1, 40, 2, 2]           9,600
      BatchNorm2d-62             [-1, 40, 2, 2]              80
 InvertedResidual-63             [-1, 40, 2, 2]               0
           Conv2d-64            [-1, 240, 2, 2]           9,600
      BatchNorm2d-65            [-1, 240, 2, 2]             480
        Hardswish-66            [-1, 240, 2, 2]               0
           Conv2d-67            [-1, 240, 2, 2]           6,000
      BatchNorm2d-68            [-1, 240, 2, 2]             480
        Hardswish-69            [-1, 240, 2, 2]               0
AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0
           Conv2d-71             [-1, 64, 1, 1]          15,424
             ReLU-72             [-1, 64, 1, 1]               0
           Conv2d-73            [-1, 240, 1, 1]          15,600
      Hardsigmoid-74            [-1, 240, 1, 1]               0
SqueezeExcitation-75            [-1, 240, 2, 2]               0
           Conv2d-76             [-1, 40, 2, 2]           9,600
      BatchNorm2d-77             [-1, 40, 2, 2]              80
 InvertedResidual-78             [-1, 40, 2, 2]               0
           Conv2d-79            [-1, 120, 2, 2]           4,800
      BatchNorm2d-80            [-1, 120, 2, 2]             240
        Hardswish-81            [-1, 120, 2, 2]               0
           Conv2d-82            [-1, 120, 2, 2]           3,000
      BatchNorm2d-83            [-1, 120, 2, 2]             240
        Hardswish-84            [-1, 120, 2, 2]               0
AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0
           Conv2d-86             [-1, 32, 1, 1]           3,872
             ReLU-87             [-1, 32, 1, 1]               0
           Conv2d-88            [-1, 120, 1, 1]           3,960
      Hardsigmoid-89            [-1, 120, 1, 1]               0
SqueezeExcitation-90            [-1, 120, 2, 2]               0
           Conv2d-91             [-1, 48, 2, 2]           5,760
      BatchNorm2d-92             [-1, 48, 2, 2]              96
 InvertedResidual-93             [-1, 48, 2, 2]               0
           Conv2d-94            [-1, 144, 2, 2]           6,912
      BatchNorm2d-95            [-1, 144, 2, 2]             288
        Hardswish-96            [-1, 144, 2, 2]               0
           Conv2d-97            [-1, 144, 2, 2]           3,600
      BatchNorm2d-98            [-1, 144, 2, 2]             288
        Hardswish-99            [-1, 144, 2, 2]               0
AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0
          Conv2d-101             [-1, 40, 1, 1]           5,800
            ReLU-102             [-1, 40, 1, 1]               0
          Conv2d-103            [-1, 144, 1, 1]           5,904
     Hardsigmoid-104            [-1, 144, 1, 1]               0
SqueezeExcitation-105            [-1, 144, 2, 2]               0
          Conv2d-106             [-1, 48, 2, 2]           6,912
     BatchNorm2d-107             [-1, 48, 2, 2]              96
InvertedResidual-108             [-1, 48, 2, 2]               0
          Conv2d-109            [-1, 288, 2, 2]          13,824
     BatchNorm2d-110            [-1, 288, 2, 2]             576
       Hardswish-111            [-1, 288, 2, 2]               0
          Conv2d-112            [-1, 288, 1, 1]           7,200
     BatchNorm2d-113            [-1, 288, 1, 1]             576
       Hardswish-114            [-1, 288, 1, 1]               0
AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0
          Conv2d-116             [-1, 72, 1, 1]          20,808
            ReLU-117             [-1, 72, 1, 1]               0
          Conv2d-118            [-1, 288, 1, 1]          21,024
     Hardsigmoid-119            [-1, 288, 1, 1]               0
SqueezeExcitation-120            [-1, 288, 1, 1]               0
          Conv2d-121             [-1, 96, 1, 1]          27,648
     BatchNorm2d-122             [-1, 96, 1, 1]             192
InvertedResidual-123             [-1, 96, 1, 1]               0
          Conv2d-124            [-1, 576, 1, 1]          55,296
     BatchNorm2d-125            [-1, 576, 1, 1]           1,152
       Hardswish-126            [-1, 576, 1, 1]               0
          Conv2d-127            [-1, 576, 1, 1]          14,400
     BatchNorm2d-128            [-1, 576, 1, 1]           1,152
       Hardswish-129            [-1, 576, 1, 1]               0
AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0
          Conv2d-131            [-1, 144, 1, 1]          83,088
            ReLU-132            [-1, 144, 1, 1]               0
          Conv2d-133            [-1, 576, 1, 1]          83,520
     Hardsigmoid-134            [-1, 576, 1, 1]               0
SqueezeExcitation-135            [-1, 576, 1, 1]               0
          Conv2d-136             [-1, 96, 1, 1]          55,296
     BatchNorm2d-137             [-1, 96, 1, 1]             192
InvertedResidual-138             [-1, 96, 1, 1]               0
          Conv2d-139            [-1, 576, 1, 1]          55,296
     BatchNorm2d-140            [-1, 576, 1, 1]           1,152
       Hardswish-141            [-1, 576, 1, 1]               0
          Conv2d-142            [-1, 576, 1, 1]          14,400
     BatchNorm2d-143            [-1, 576, 1, 1]           1,152
       Hardswish-144            [-1, 576, 1, 1]               0
AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0
          Conv2d-146            [-1, 144, 1, 1]          83,088
            ReLU-147            [-1, 144, 1, 1]               0
          Conv2d-148            [-1, 576, 1, 1]          83,520
     Hardsigmoid-149            [-1, 576, 1, 1]               0
SqueezeExcitation-150            [-1, 576, 1, 1]               0
          Conv2d-151             [-1, 96, 1, 1]          55,296
     BatchNorm2d-152             [-1, 96, 1, 1]             192
InvertedResidual-153             [-1, 96, 1, 1]               0
          Conv2d-154            [-1, 576, 1, 1]          55,296
     BatchNorm2d-155            [-1, 576, 1, 1]           1,152
       Hardswish-156            [-1, 576, 1, 1]               0
AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0
          Linear-158                 [-1, 1024]         590,848
       Hardswish-159                 [-1, 1024]               0
         Dropout-160                 [-1, 1024]               0
          Linear-161                   [-1, 10]          10,250
================================================================
Total params: 1,528,106
Trainable params: 1,528,106
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.79
Params size (MB): 5.83
Estimated Total Size (MB): 6.63
----------------------------------------------------------------
[32mMobileNetV3(
[32m  (features): Sequential(
[32m    (0): Conv2dNormActivation(
[32m      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
[32m      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m      (2): Hardswish()
[32m    )
[32m    (1): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)
[32m          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): ReLU(inplace=True)
[32m        )
[32m        (1): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (2): Conv2dNormActivation(
[32m          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (2): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): ReLU(inplace=True)
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)
[32m          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): ReLU(inplace=True)
[32m        )
[32m        (2): Conv2dNormActivation(
[32m          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (3): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): ReLU(inplace=True)
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)
[32m          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): ReLU(inplace=True)
[32m        )
[32m        (2): Conv2dNormActivation(
[32m          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (4): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
[32m          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (5): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
[32m          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (6): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
[32m          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (7): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
[32m          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (8): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)
[32m          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (9): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
[32m          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (10): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
[32m          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (11): InvertedResidual(
[32m      (block): Sequential(
[32m        (0): Conv2dNormActivation(
[32m          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (1): Conv2dNormActivation(
[32m          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
[32m          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m          (2): Hardswish()
[32m        )
[32m        (2): SqueezeExcitation(
[32m          (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))
[32m          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))
[32m          (activation): ReLU()
[32m          (scale_activation): Hardsigmoid()
[32m        )
[32m        (3): Conv2dNormActivation(
[32m          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m        )
[32m      )
[32m    )
[32m    (12): Conv2dNormActivation(
[32m      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
[32m      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
[32m      (2): Hardswish()
[32m    )
[32m  )
[32m  (avgpool): AdaptiveAvgPool2d(output_size=1)
[32m  (classifier): Sequential(
[32m    (0): Linear(in_features=576, out_features=1024, bias=True)
[32m    (1): Hardswish()
[32m    (2): Dropout(p=0.2, inplace=True)
[32m    (3): Linear(in_features=1024, out_features=10, bias=True)
[32m  )
[32m)
åˆå§‹åŒ–çš„å­¦ä¹ ç‡ï¼š 0.001
[34må¼€å§‹è®­ç»ƒ......
ç¬¬1ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.001000
epoch:1/100 	 train_acc:43.79399871826172 	 val_acc:55.019996643066406 	 train_loss:1.0973049402236938 	 val_loss:0.9726960062980652
ç¬¬2ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.001000
epoch:2/100 	 train_acc:57.48599624633789 	 val_acc:60.71999740600586 	 train_loss:1.2682678699493408 	 val_loss:0.8021596670150757
ç¬¬3ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000999
epoch:3/100 	 train_acc:62.19599914550781 	 val_acc:64.62000274658203 	 train_loss:0.9725871086120605 	 val_loss:0.7372360229492188
ç¬¬4ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000998
epoch:4/100 	 train_acc:65.06199645996094 	 val_acc:67.5999984741211 	 train_loss:0.8230220079421997 	 val_loss:0.82142174243927
ç¬¬5ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000996
epoch:5/100 	 train_acc:66.85999298095703 	 val_acc:67.83999633789062 	 train_loss:0.9685389399528503 	 val_loss:0.7220105528831482
ç¬¬6ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000994
epoch:6/100 	 train_acc:68.75199890136719 	 val_acc:69.08999633789062 	 train_loss:0.8468057513237 	 val_loss:0.6908199787139893
ç¬¬7ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000991
epoch:7/100 	 train_acc:69.7979965209961 	 val_acc:68.97999572753906 	 train_loss:0.8673499822616577 	 val_loss:0.6783064007759094
ç¬¬8ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000988
epoch:8/100 	 train_acc:70.77799987792969 	 val_acc:71.37000274658203 	 train_loss:0.9026721119880676 	 val_loss:0.6602134108543396
ç¬¬9ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000984
epoch:9/100 	 train_acc:71.56400299072266 	 val_acc:70.91999816894531 	 train_loss:0.6604487299919128 	 val_loss:0.44989705085754395
ç¬¬10ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000980
epoch:10/100 	 train_acc:72.53599548339844 	 val_acc:72.31999969482422 	 train_loss:0.841733455657959 	 val_loss:0.5712433457374573
ç¬¬11ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000976
epoch:11/100 	 train_acc:73.10199737548828 	 val_acc:72.06999969482422 	 train_loss:1.0190144777297974 	 val_loss:0.7103045582771301
ç¬¬12ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000970
epoch:12/100 	 train_acc:73.7760009765625 	 val_acc:73.03999328613281 	 train_loss:0.7169609069824219 	 val_loss:0.6526605486869812
ç¬¬13ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000965
epoch:13/100 	 train_acc:74.63399505615234 	 val_acc:71.93000030517578 	 train_loss:0.7377822399139404 	 val_loss:0.6840010285377502
ç¬¬14ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000959
epoch:14/100 	 train_acc:75.00599670410156 	 val_acc:73.05999755859375 	 train_loss:0.6321350336074829 	 val_loss:0.9291841983795166
ç¬¬15ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000952
epoch:15/100 	 train_acc:75.47799682617188 	 val_acc:71.31999969482422 	 train_loss:0.8302401304244995 	 val_loss:0.6418341994285583
ç¬¬16ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000946
epoch:16/100 	 train_acc:75.69999694824219 	 val_acc:72.5199966430664 	 train_loss:0.5323189496994019 	 val_loss:0.6719342470169067
ç¬¬17ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000938
epoch:17/100 	 train_acc:76.3740005493164 	 val_acc:73.18999481201172 	 train_loss:0.7332445383071899 	 val_loss:0.6217436194419861
ç¬¬18ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000930
epoch:18/100 	 train_acc:77.22000122070312 	 val_acc:73.98999786376953 	 train_loss:0.7599450349807739 	 val_loss:0.53565913438797
ç¬¬19ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000922
epoch:19/100 	 train_acc:77.5 	 val_acc:73.48999786376953 	 train_loss:0.9127160906791687 	 val_loss:0.6014419198036194
ç¬¬20ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000914
epoch:20/100 	 train_acc:78.05799865722656 	 val_acc:73.40999603271484 	 train_loss:0.5643795728683472 	 val_loss:0.704371452331543
ç¬¬21ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000905
epoch:21/100 	 train_acc:78.31599426269531 	 val_acc:73.82999420166016 	 train_loss:0.6716188192367554 	 val_loss:0.850829541683197
ç¬¬22ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000895
epoch:22/100 	 train_acc:78.87999725341797 	 val_acc:74.72999572753906 	 train_loss:0.7698297500610352 	 val_loss:0.8170338273048401
ç¬¬23ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000885
epoch:23/100 	 train_acc:79.47999572753906 	 val_acc:74.23999786376953 	 train_loss:0.44338375329971313 	 val_loss:0.5544196367263794
ç¬¬24ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000875
epoch:24/100 	 train_acc:80.01599884033203 	 val_acc:74.2699966430664 	 train_loss:0.5572085380554199 	 val_loss:0.568834125995636
ç¬¬25ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000864
epoch:25/100 	 train_acc:80.77999877929688 	 val_acc:74.62999725341797 	 train_loss:0.6111332774162292 	 val_loss:0.671541690826416
ç¬¬26ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000854
epoch:26/100 	 train_acc:81.01599884033203 	 val_acc:73.6199951171875 	 train_loss:0.6855751276016235 	 val_loss:0.7942190766334534
ç¬¬27ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000842
epoch:27/100 	 train_acc:81.02200317382812 	 val_acc:74.02999877929688 	 train_loss:0.5618318915367126 	 val_loss:0.7156593799591064
ç¬¬28ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000831
epoch:28/100 	 train_acc:81.54199981689453 	 val_acc:74.65999603271484 	 train_loss:0.4770170748233795 	 val_loss:0.7792598605155945
ç¬¬29ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000819
epoch:29/100 	 train_acc:82.21199798583984 	 val_acc:74.64999389648438 	 train_loss:0.5447646379470825 	 val_loss:0.7523394227027893
ç¬¬30ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000806
epoch:30/100 	 train_acc:82.7760009765625 	 val_acc:74.68000030517578 	 train_loss:0.6235843896865845 	 val_loss:0.8078939318656921
ç¬¬31ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000794
epoch:31/100 	 train_acc:83.06999969482422 	 val_acc:74.52999877929688 	 train_loss:0.4773373603820801 	 val_loss:0.7242865562438965
ç¬¬32ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000781
epoch:32/100 	 train_acc:83.49600219726562 	 val_acc:73.94999694824219 	 train_loss:0.395199716091156 	 val_loss:0.6964022517204285
ç¬¬33ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000768
epoch:33/100 	 train_acc:84.0999984741211 	 val_acc:74.25 	 train_loss:0.4076569676399231 	 val_loss:0.712317943572998
ç¬¬34ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000755
epoch:34/100 	 train_acc:84.59199523925781 	 val_acc:74.98999786376953 	 train_loss:0.4757606089115143 	 val_loss:0.3906530439853668
ç¬¬35ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000741
epoch:35/100 	 train_acc:84.98599243164062 	 val_acc:74.47999572753906 	 train_loss:0.6097984313964844 	 val_loss:0.5951183438301086
ç¬¬36ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000727
epoch:36/100 	 train_acc:85.13400268554688 	 val_acc:73.8699951171875 	 train_loss:0.3275049030780792 	 val_loss:0.8411553502082825
ç¬¬37ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000713
epoch:37/100 	 train_acc:85.80999755859375 	 val_acc:75.04999542236328 	 train_loss:0.3513936698436737 	 val_loss:0.9769924879074097
ç¬¬38ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000699
epoch:38/100 	 train_acc:85.85599517822266 	 val_acc:74.44999694824219 	 train_loss:0.5517722964286804 	 val_loss:0.5661945939064026
ç¬¬39ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000684
epoch:39/100 	 train_acc:86.60599517822266 	 val_acc:73.79999542236328 	 train_loss:0.2387201339006424 	 val_loss:0.638802170753479
ç¬¬40ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000669
epoch:40/100 	 train_acc:86.60800170898438 	 val_acc:74.0 	 train_loss:0.6255833506584167 	 val_loss:0.40430739521980286
ç¬¬41ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000655
epoch:41/100 	 train_acc:87.2979965209961 	 val_acc:74.79000091552734 	 train_loss:0.3076775074005127 	 val_loss:0.6704167127609253
ç¬¬42ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000639
epoch:42/100 	 train_acc:87.84600067138672 	 val_acc:74.0999984741211 	 train_loss:0.46641620993614197 	 val_loss:0.5462366938591003
ç¬¬43ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000624
epoch:43/100 	 train_acc:88.25799560546875 	 val_acc:74.70999908447266 	 train_loss:0.32243257761001587 	 val_loss:0.4703315496444702
ç¬¬44ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000609
epoch:44/100 	 train_acc:88.34199523925781 	 val_acc:74.90999603271484 	 train_loss:0.18469248712062836 	 val_loss:0.8658248782157898
ç¬¬45ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000594
epoch:45/100 	 train_acc:87.77799987792969 	 val_acc:75.0 	 train_loss:0.372511088848114 	 val_loss:0.8905569314956665
ç¬¬46ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000578
epoch:46/100 	 train_acc:88.58399963378906 	 val_acc:75.08999633789062 	 train_loss:0.34958094358444214 	 val_loss:1.0638982057571411
ç¬¬47ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000563
epoch:47/100 	 train_acc:89.62999725341797 	 val_acc:74.79000091552734 	 train_loss:0.3980230689048767 	 val_loss:0.6688406467437744
ç¬¬48ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000547
epoch:48/100 	 train_acc:89.80799865722656 	 val_acc:75.37999725341797 	 train_loss:0.5042837858200073 	 val_loss:0.6715843081474304
ç¬¬49ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000531
epoch:49/100 	 train_acc:90.17799377441406 	 val_acc:74.65999603271484 	 train_loss:0.42870616912841797 	 val_loss:0.7136857509613037
ç¬¬50ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000516
epoch:50/100 	 train_acc:90.44000244140625 	 val_acc:74.4699935913086 	 train_loss:1.197148084640503 	 val_loss:0.8391619324684143
ç¬¬51ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000500
epoch:51/100 	 train_acc:89.72000122070312 	 val_acc:75.23999786376953 	 train_loss:0.2534678876399994 	 val_loss:0.5319978594779968
ç¬¬52ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000484
epoch:52/100 	 train_acc:91.1199951171875 	 val_acc:74.61000061035156 	 train_loss:0.3216584026813507 	 val_loss:0.7082700133323669
ç¬¬53ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000469
epoch:53/100 	 train_acc:91.08799743652344 	 val_acc:69.55999755859375 	 train_loss:0.34439563751220703 	 val_loss:0.7912695407867432
ç¬¬54ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000453
epoch:54/100 	 train_acc:90.2040023803711 	 val_acc:74.70999908447266 	 train_loss:0.16256549954414368 	 val_loss:0.8131574392318726
ç¬¬55ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000437
epoch:55/100 	 train_acc:91.76399993896484 	 val_acc:75.29000091552734 	 train_loss:0.2608928680419922 	 val_loss:0.6332785487174988
ç¬¬56ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000422
epoch:56/100 	 train_acc:92.0219955444336 	 val_acc:74.97999572753906 	 train_loss:0.29774653911590576 	 val_loss:0.8789508938789368
ç¬¬57ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000406
epoch:57/100 	 train_acc:92.4020004272461 	 val_acc:74.97000122070312 	 train_loss:0.40994149446487427 	 val_loss:0.971626877784729
ç¬¬58ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000391
epoch:58/100 	 train_acc:92.69400024414062 	 val_acc:75.3699951171875 	 train_loss:0.2539287805557251 	 val_loss:0.9250971078872681
ç¬¬59ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000376
epoch:59/100 	 train_acc:92.91600036621094 	 val_acc:75.06999969482422 	 train_loss:0.23124685883522034 	 val_loss:1.0026156902313232
ç¬¬60ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000361
epoch:60/100 	 train_acc:92.9699935913086 	 val_acc:74.7699966430664 	 train_loss:0.16148681938648224 	 val_loss:0.9038866758346558
ç¬¬61ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000345
epoch:61/100 	 train_acc:93.2199935913086 	 val_acc:75.00999450683594 	 train_loss:0.1316373646259308 	 val_loss:0.9638782739639282
ç¬¬62ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000331
epoch:62/100 	 train_acc:93.36799621582031 	 val_acc:75.43000030517578 	 train_loss:0.15218861401081085 	 val_loss:1.237560749053955
ç¬¬63ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000316
epoch:63/100 	 train_acc:93.6939926147461 	 val_acc:74.54999542236328 	 train_loss:0.149069145321846 	 val_loss:0.8541528582572937
ç¬¬64ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000301
epoch:64/100 	 train_acc:93.89799499511719 	 val_acc:75.00999450683594 	 train_loss:0.18819892406463623 	 val_loss:1.0208858251571655
ç¬¬65ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000287
epoch:65/100 	 train_acc:94.1760025024414 	 val_acc:75.29000091552734 	 train_loss:0.16656264662742615 	 val_loss:1.3485924005508423
ç¬¬66ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000273
epoch:66/100 	 train_acc:94.25 	 val_acc:75.23999786376953 	 train_loss:0.22842316329479218 	 val_loss:1.337969183921814
ç¬¬67ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000259
epoch:67/100 	 train_acc:94.55599975585938 	 val_acc:74.66999816894531 	 train_loss:0.23794850707054138 	 val_loss:0.9211326837539673
ç¬¬68ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000245
epoch:68/100 	 train_acc:94.48200225830078 	 val_acc:75.15999603271484 	 train_loss:0.18927595019340515 	 val_loss:1.0565893650054932
ç¬¬69ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000232
epoch:69/100 	 train_acc:94.89199829101562 	 val_acc:75.27999877929688 	 train_loss:0.258907675743103 	 val_loss:1.057602047920227
ç¬¬70ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000219
epoch:70/100 	 train_acc:94.83200073242188 	 val_acc:75.1199951171875 	 train_loss:0.12236256897449493 	 val_loss:1.1306020021438599
ç¬¬71ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000206
epoch:71/100 	 train_acc:95.04399871826172 	 val_acc:75.2699966430664 	 train_loss:0.04463482275605202 	 val_loss:1.0316238403320312
ç¬¬72ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000194
epoch:72/100 	 train_acc:95.23999786376953 	 val_acc:75.2699966430664 	 train_loss:0.12925438582897186 	 val_loss:1.1981799602508545
ç¬¬73ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000181
epoch:73/100 	 train_acc:95.19000244140625 	 val_acc:75.15999603271484 	 train_loss:0.42956191301345825 	 val_loss:0.9575246572494507
ç¬¬74ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000169
epoch:74/100 	 train_acc:95.3479995727539 	 val_acc:75.29000091552734 	 train_loss:0.09645333141088486 	 val_loss:0.9746257662773132
ç¬¬75ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000158
epoch:75/100 	 train_acc:95.55599975585938 	 val_acc:75.29000091552734 	 train_loss:0.16050459444522858 	 val_loss:1.076684832572937
ç¬¬76ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000146
epoch:76/100 	 train_acc:95.57199096679688 	 val_acc:75.50999450683594 	 train_loss:0.08085109293460846 	 val_loss:1.0378944873809814
ç¬¬77ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000136
epoch:77/100 	 train_acc:95.79000091552734 	 val_acc:75.0999984741211 	 train_loss:0.1400691717863083 	 val_loss:1.0551186800003052
ç¬¬78ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000125
epoch:78/100 	 train_acc:95.91400146484375 	 val_acc:75.22999572753906 	 train_loss:0.1021757572889328 	 val_loss:0.9903804659843445
ç¬¬79ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000115
epoch:79/100 	 train_acc:96.05799865722656 	 val_acc:75.45999908447266 	 train_loss:0.08942413330078125 	 val_loss:1.1351428031921387
ç¬¬80ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000105
epoch:80/100 	 train_acc:96.0719985961914 	 val_acc:75.2699966430664 	 train_loss:0.17423486709594727 	 val_loss:1.1836979389190674
ç¬¬81ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000095
epoch:81/100 	 train_acc:96.27799987792969 	 val_acc:75.44999694824219 	 train_loss:0.12048240005970001 	 val_loss:1.1081087589263916
ç¬¬82ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000086
epoch:82/100 	 train_acc:96.1240005493164 	 val_acc:75.40999603271484 	 train_loss:0.20463454723358154 	 val_loss:1.103633999824524
ç¬¬83ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000078
epoch:83/100 	 train_acc:96.36599731445312 	 val_acc:75.33999633789062 	 train_loss:0.13715478777885437 	 val_loss:1.1240248680114746
ç¬¬84ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000070
epoch:84/100 	 train_acc:96.27999877929688 	 val_acc:75.36000061035156 	 train_loss:0.1201220154762268 	 val_loss:1.0398060083389282
ç¬¬85ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000062
epoch:85/100 	 train_acc:96.33599853515625 	 val_acc:75.4000015258789 	 train_loss:0.5233845710754395 	 val_loss:1.111643671989441
ç¬¬86ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000054
epoch:86/100 	 train_acc:96.46399688720703 	 val_acc:75.50999450683594 	 train_loss:0.13423889875411987 	 val_loss:1.0831893682479858
ç¬¬87ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000048
epoch:87/100 	 train_acc:96.46599578857422 	 val_acc:75.27999877929688 	 train_loss:0.0929902046918869 	 val_loss:1.0296361446380615
ç¬¬88ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000041
epoch:88/100 	 train_acc:96.46399688720703 	 val_acc:75.31999969482422 	 train_loss:0.12746195495128632 	 val_loss:1.072731375694275
ç¬¬89ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000035
epoch:89/100 	 train_acc:96.60399627685547 	 val_acc:75.3699951171875 	 train_loss:0.09898798167705536 	 val_loss:1.1201454401016235
ç¬¬90ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000030
epoch:90/100 	 train_acc:96.69999694824219 	 val_acc:75.32999420166016 	 train_loss:0.14301709830760956 	 val_loss:1.107667088508606
ç¬¬91ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000024
epoch:91/100 	 train_acc:96.64199829101562 	 val_acc:75.29999542236328 	 train_loss:0.12172938883304596 	 val_loss:1.0479358434677124
ç¬¬92ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000020
epoch:92/100 	 train_acc:96.66400146484375 	 val_acc:75.31999969482422 	 train_loss:0.1233006939291954 	 val_loss:1.0811830759048462
ç¬¬93ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000016
epoch:93/100 	 train_acc:96.66400146484375 	 val_acc:75.47999572753906 	 train_loss:0.1956891417503357 	 val_loss:1.0621321201324463
ç¬¬94ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000012
epoch:94/100 	 train_acc:96.83399963378906 	 val_acc:75.41999816894531 	 train_loss:0.16445967555046082 	 val_loss:1.0938607454299927
ç¬¬95ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000009
epoch:95/100 	 train_acc:96.80199432373047 	 val_acc:75.38999938964844 	 train_loss:0.06390362977981567 	 val_loss:1.1073698997497559
ç¬¬96ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000006
epoch:96/100 	 train_acc:96.74199676513672 	 val_acc:75.5 	 train_loss:0.09982867538928986 	 val_loss:1.0899786949157715
ç¬¬97ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000004
epoch:97/100 	 train_acc:96.83199310302734 	 val_acc:75.4000015258789 	 train_loss:0.06917273253202438 	 val_loss:1.1031959056854248
ç¬¬98ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000002
epoch:98/100 	 train_acc:97.01199340820312 	 val_acc:75.50999450683594 	 train_loss:0.2412727326154709 	 val_loss:1.0800868272781372
ç¬¬99ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000001
epoch:99/100 	 train_acc:96.77799987792969 	 val_acc:75.50999450683594 	 train_loss:0.12237272411584854 	 val_loss:1.0982716083526611
ç¬¬100ä¸ªepochçš„å­¦ä¹ ç‡ï¼š0.000000
epoch:100/100 	 train_acc:96.73999786376953 	 val_acc:75.50999450683594 	 train_loss:0.09054284542798996 	 val_loss:1.0984413623809814
æ¨¡å‹ä¿å­˜æˆåŠŸ:model.pth
è®­ç»ƒå®Œæˆ